{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QAGQmhmU1z3z"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "eng_hin ='English_Hindi_Clean_New.csv'\n",
    "data=pd.read_csv(eng_hin, encoding='utf-8')\n",
    "# Get English and Hindi Vocabulary\n",
    "all_eng_words = set()\n",
    "for eng in data['English']:\n",
    "    for word in eng.split():\n",
    "        if word not in all_eng_words:\n",
    "            all_eng_words.add(word)\n",
    "\n",
    "all_hin_words = set()\n",
    "for hin in data['Hindi']:\n",
    "    for word in hin.split():\n",
    "        if word not in all_hin_words:\n",
    "            all_hin_words.add(word)\n",
    "\n",
    "data['len_eng_sen'] = data['English'].apply(lambda x: len(x.split(\" \")))\n",
    "data['len_hin_sen'] = data['Hindi'].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "data = data[data['len_eng_sen'] <= 20]\n",
    "data = data[data['len_hin_sen'] <= 20]\n",
    "\n",
    "max_len_src = max(data['len_hin_sen'])\n",
    "max_len_tar = max(data['len_eng_sen'])\n",
    "\n",
    "inp_words = sorted(list(all_eng_words))\n",
    "tar_words = sorted(list(all_hin_words))\n",
    "num_enc_toks = len(all_eng_words)\n",
    "num_dec_toks = len(all_hin_words) + 1  # for zero padding\n",
    "\n",
    "inp_tok_idx = dict((word, i + 1) for i, word in enumerate(inp_words))\n",
    "tar_tok_idx = dict((word, i + 1) for i, word in enumerate(tar_words))\n",
    "rev_inp_char_idx = dict((i, word) for word, i in inp_tok_idx.items())\n",
    "rev_tar_char_idx = dict((i, word) for word, i in tar_tok_idx.items())\n",
    "# Split the data into train and test\n",
    "X, y = data['English'], data['Hindi']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Increase batch size\n",
    "batch_size = 256\n",
    "\n",
    "# Generate batch data\n",
    "def generate_batch(X=X_train, y=y_train, batch_size=batch_size):\n",
    "    while True:\n",
    "        for j in range(0, len(X), batch_size):\n",
    "            enc_inp_data = np.zeros((batch_size, max_len_src), dtype='float32')\n",
    "            dec_inp_data = np.zeros((batch_size, max_len_tar), dtype='float32')\n",
    "            dec_tar_data = np.zeros((batch_size, max_len_tar, num_dec_toks), dtype='float32')\n",
    "            for i, (inp_text, tar_text) in enumerate(zip(X[j:j + batch_size], y[j:j + batch_size])):\n",
    "                for t, word in enumerate(inp_text.split()):\n",
    "                    enc_inp_data[i, t] = inp_tok_idx[word]\n",
    "                for t, word in enumerate(tar_text.split()):\n",
    "                    if t < len(tar_text.split()) - 1:\n",
    "                        dec_inp_data[i, t] = tar_tok_idx[word]\n",
    "                    if t > 0:\n",
    "                        dec_tar_data[i, t - 1, tar_tok_idx[word]] = 1.0\n",
    "            yield [enc_inp_data, dec_inp_data], dec_tar_data\n",
    "\n",
    "# Encoder-Decoder Architecture\n",
    "latent_dim = 250\n",
    "\n",
    "# Encoder\n",
    "enc_inps = Input(shape=(None,))\n",
    "enc_emb = Embedding(num_enc_toks, latent_dim, mask_zero=True)(enc_inps)\n",
    "enc_lstm = LSTM(latent_dim, return_state=True)\n",
    "enc_outputs, st_h, st_c = enc_lstm(enc_emb)\n",
    "enc_states = [st_h, st_c]\n",
    "\n",
    "# Set up the decoder\n",
    "dec_inps = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(num_dec_toks, latent_dim, mask_zero=True)\n",
    "dec_emb = dec_emb_layer(dec_inps)\n",
    "dec_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "dec_outputs, _, _ = dec_lstm(dec_emb, initial_state=enc_states)\n",
    "dec_dense = Dense(num_dec_toks, activation='softmax')\n",
    "dec_outputs = dec_dense(dec_outputs)\n",
    "# Define the model\n",
    "model = Model([enc_inps, dec_inps], dec_outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')  # Use Adam optimizer for faster convergence\n",
    "train_samples = len(X_train)\n",
    "val_samples = len(X_test)\n",
    "\n",
    "# Train the model with a larger batch size\n",
    "model.fit(x=generate_batch(X_train, y_train, batch_size=batch_size),\n",
    "          steps_per_epoch=train_samples // batch_size,\n",
    "          epochs=50,\n",
    "          validation_data=generate_batch(X_test, y_test, batch_size=batch_size),\n",
    "          validation_steps=val_samples // batch_size)\n",
    "# Encode the input sequence to get the \"thought vectors\"\n",
    "enc_model = Model(enc_inps, enc_states)\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "dec_st_inp_h = Input(shape=(latent_dim,))\n",
    "dec_st_inp_c = Input(shape=(latent_dim,))\n",
    "dec_states_inps = [dec_st_inp_h, dec_st_inp_c]\n",
    "\n",
    "dec_emb2= dec_emb_layer(dec_inps) # Get the embeddings of the decoder sequence\n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states \n",
    "#from the previous time step\n",
    "dec_outputs2, st_h2, st_c2 = dec_lstm(dec_emb2, initial_state=dec_states_inps)\n",
    "dec_states2 = [st_h2, st_c2]\n",
    "dec_outputs2 = dec_dense(dec_outputs2) # A dense softmax layer to generate prob dist\n",
    "#over the target vocabulary\n",
    "\n",
    "# Final decoder model\n",
    "dec_model = Model(\n",
    "    [dec_inps] + dec_states_inps,\n",
    "    [dec_outputs2] + dec_states2)\n",
    "def translate(inp_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = enc_model.predict(inp_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    tar_seq = np.zeros((1,1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    tar_seq[0, 0] = tar_tok_idx['START_']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_cond = False\n",
    "    dec_sen = ''\n",
    "    while not stop_cond:\n",
    "        output_toks, h, c = dec_model.predict([tar_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_tok_idx = np.argmax(output_toks[0, -1, :])\n",
    "        sampled_char = rev_tar_char_idx[sampled_tok_idx]\n",
    "        dec_sen += ' '+sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '_END' or\n",
    "           len(dec_sen) > 50):\n",
    "            stop_cond = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        tar_seq = np.zeros((1,1))\n",
    "        tar_seq[0, 0] = sampled_tok_idx\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return dec_sen\n",
    "\n",
    "train_gen = generate_batch(X_train, y_train, batch_size = 1)\n",
    "k=0\n",
    "(inp_seq, actual_output), _ = next(train_gen)\n",
    "hin_sen = translate(inp_seq)\n",
    "print(f'''Input English sentence: {X_train[k:k+1].values[0]}\\n\n",
    "          Predicted Hindi Translation: {hin_sen[:-4]}\\n\n",
    "          Actual Hindi Translation: {y_train[k:k+1].values[0][6:-4]}''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bvw8A0yI2nBJ",
    "outputId": "feaa32b4-121f-4014-d98c-f0996bdacd54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Input English sentence: which is a pity but in india every other sport\n",
      "\n",
      "          Predicted Hindi Translation:  जिसपे हमें तरस आती है कि भारत में एक एकल \n",
      "\n",
      "          Actual Hindi Translation:  जिसपे हमें तरस आती है लेकिन भारत में हर खेल \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "NcTpFSOf5UTc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
